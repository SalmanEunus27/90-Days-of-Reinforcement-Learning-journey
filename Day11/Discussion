ðŸŒŸ **Day 11 of 90 Days of Reinforcement Learning**

**Topic:** *Temporal Difference (TD) Learning*

So far, weâ€™ve discussed **value functions** and the **Bellman Equation**.
But how do agents **actually learn** these values from experience?
Thatâ€™s where **Temporal Difference Learning** comes in.

---

ðŸ”¹ **What is TD Learning?**
TD Learning is a method where an agent **updates its estimates of value functions based on partial information**, without waiting for the outcome.
It combines ideas from:

* **Monte Carlo methods:** Learning from complete episodes
* **Dynamic Programming:** Bootstrapping from current estimates

---

ðŸ’¡ **TD Update Rule:**
[
V(s) = V(s) + \alpha [r + \gamma V(s') - V(s)]
]

* **Î± (alpha):** Learning rate
* **r:** Reward received
* **Î³ (gamma):** Discount factor
* **s':** Next state

---

ðŸ”¹ **Why itâ€™s powerful:**

* Learns *in real-time*, step-by-step.
* Forms the foundation of famous algorithms like **SARSA** and **Q-Learning**.
* Great for situations where the environment is **continuous** or **never-ending**.

ðŸ”‘ **Takeaway:**
Temporal Difference Learning is the **bridge between theory and practice** â€” allowing agents to **learn directly from raw experience**.

#ReinforcementLearning #MachineLearning #AI #90DaysOfRL #LearningInPublic #TemporalDifference
